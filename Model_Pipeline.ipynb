{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c799a82",
   "metadata": {},
   "source": [
    "# **Air Pollution Forecasting Model**  \n",
    "## **Hybrid LSTM-GRU-Transformer Model for PM2.5 Prediction**  \n",
    "*This notebook implements a deep learning pipeline for forecasting air pollution levels, specifically PM2.5 concentrations. The approach integrates LSTM, GRU, and Transformer models to capture temporal dependencies in the data.*  \n",
    "\n",
    "---\n",
    "### **Objectives of This Notebook:**  \n",
    "- Load and preprocess time-series air pollution data.  \n",
    "- Build a hybrid model combining LSTM, GRU, and Transformer architectures.  \n",
    "- Train the model using historical air pollution data.  \n",
    "- Evaluate model performance using key metrics such as R², MAE, and RMSE.  \n",
    "- Save the trained model for future use.  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a67ec",
   "metadata": {},
   "source": [
    "## **Step 1: Import Required Libraries**  \n",
    "To build and train the model efficiently, we import necessary libraries:  \n",
    "- **`pandas`**: For data loading and preprocessing.  \n",
    "- **`tensorflow.keras`**: For deep learning model creation.  \n",
    "- **`sklearn.preprocessing`**: For feature scaling.  \n",
    "- **`sklearn.metrics`**: For model evaluation.  \n",
    "These libraries form the core framework for our machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, GRU, Dense, Concatenate, MultiHeadAttention,\n",
    "    LayerNormalization, Dropout, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d153e",
   "metadata": {},
   "source": [
    "## **Step 2: Load and Inspect the Dataset**  \n",
    "The dataset contains air pollution time-series data with various pollutant concentrations.  \n",
    "**Key Actions in this Step:**  \n",
    "- Load the preprocessed dataset from a CSV file.  \n",
    "- Display the first five rows to inspect its structure.  \n",
    "- Identify feature columns that will be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Timestamp  PM2.5 (µg/m³)  PM10 (µg/m³)  NO (µg/m³)  NO2 (µg/m³)  \\\n",
      "0  2017-01-01       230.5000      329.4500       14.22        11.21   \n",
      "1  2017-01-02       229.6352      328.2442       14.22        11.21   \n",
      "2  2017-01-03       228.7705      327.0383       14.22        11.21   \n",
      "3  2017-01-04       227.9057      325.8325       14.22        11.21   \n",
      "4  2017-01-05       227.0409      324.6267       14.22        11.21   \n",
      "\n",
      "   NOx (ppb)  NH3 (µg/m³)  SO2 (µg/m³)  CO (mg/m³)  Ozone (µg/m³)  ...  \\\n",
      "0    25.4300          NaN          NaN      0.7400        56.5000  ...   \n",
      "1    25.5190          NaN          NaN      0.7421        56.3816  ...   \n",
      "2    25.6081          NaN          NaN      0.7441        56.2632  ...   \n",
      "3    25.6971          NaN          NaN      0.7462        56.1449  ...   \n",
      "4    25.7862          NaN          NaN      0.7483        56.0265  ...   \n",
      "\n",
      "   PM2.5 (µg/m³)_rolling_mean  PM2.5 (µg/m³)_lag_1  PM10 (µg/m³)_rolling_mean  \\\n",
      "0                    230.5000             230.5000                   329.4500   \n",
      "1                    230.0676             230.5000                   328.8471   \n",
      "2                    229.6352             229.6352                   328.2442   \n",
      "3                    229.2028             228.7705                   327.6412   \n",
      "4                    228.7705             227.9057                   327.0383   \n",
      "\n",
      "   PM10 (µg/m³)_lag_1  NOx (ppb)_rolling_mean  NOx (ppb)_lag_1  \\\n",
      "0            329.4500                 25.4300          25.4300   \n",
      "1            329.4500                 25.4745          25.4300   \n",
      "2            328.2442                 25.5190          25.5190   \n",
      "3            327.0383                 25.5636          25.6081   \n",
      "4            325.8325                 25.6081          25.6971   \n",
      "\n",
      "   CO (mg/m³)_rolling_mean  CO (mg/m³)_lag_1  Ozone (µg/m³)_rolling_mean  \\\n",
      "0                   0.7400            0.7400                     56.5000   \n",
      "1                   0.7410            0.7400                     56.4408   \n",
      "2                   0.7421            0.7421                     56.3816   \n",
      "3                   0.7431            0.7441                     56.3224   \n",
      "4                   0.7441            0.7462                     56.2632   \n",
      "\n",
      "   Ozone (µg/m³)_lag_1  \n",
      "0              56.5000  \n",
      "1              56.5000  \n",
      "2              56.3816  \n",
      "3              56.2632  \n",
      "4              56.1449  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = \"./data\"\n",
    "MODEL_DIR = \"./models\"\n",
    "INPUT_FILE = os.path.join(DATA_DIR, \"Enhanced_Time-Series_Air_Pollution_Data_Revised.csv\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"base_model.keras\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "print(data.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13439990",
   "metadata": {},
   "source": [
    "## **Step 3: Feature Selection and Data Normalization**  \n",
    "In this step, we define input features and the target variable (`PM2.5`).  \n",
    "**Transformations Applied:**  \n",
    "- **Min-Max Scaling:** Standardizes input features to the range `[0,1]` for stable model training.  \n",
    "- **Reshaping Data:** Ensures the input dimensions align with TensorFlow's LSTM/GRU requirements.  \n",
    "- **Train-Test Split:** Splits the data into training (80%) and testing (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = [\n",
    "    'PM10 (µg/m³)', 'NOx (ppb)', 'CO (mg/m³)', 'Ozone (µg/m³)',\n",
    "    'PM2.5 (µg/m³)_rolling_mean', 'PM2.5 (µg/m³)_lag_1',\n",
    "    'PM10 (µg/m³)_rolling_mean', 'PM10 (µg/m³)_lag_1',\n",
    "    'NOx (ppb)_rolling_mean', 'NOx (ppb)_lag_1',\n",
    "    'CO (mg/m³)_rolling_mean', 'CO (mg/m³)_lag_1',\n",
    "    'Ozone (µg/m³)_rolling_mean', 'Ozone (µg/m³)_lag_1'\n",
    "]\n",
    "target = 'PM2.5 (µg/m³)'\n",
    "\n",
    "# Normalize features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(data[features])\n",
    "y = scaler_y.fit_transform(data[target].values.reshape(-1, 1))\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183c4e4",
   "metadata": {},
   "source": [
    "## **Step 4: Define the Hybrid LSTM-GRU-Transformer Model**  \n",
    "We design a hybrid deep learning model with three key components:  \n",
    "- **LSTM Branch:** Captures long-term dependencies in the time series.  \n",
    "- **GRU Branch:** Captures short-term dependencies while reducing computational cost.  \n",
    "- **Transformer Encoder:** Uses multi-head attention to model complex temporal relationships.  \n",
    "**Key Layers Used:**  \n",
    "- **`MultiHeadAttention`**: Enhances feature extraction in the Transformer branch.  \n",
    "- **`Dropout` & `LayerNormalization`**: Prevent overfitting and stabilize learning.  \n",
    "- **`Dense` Layers**: Perform final feature transformation before prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def define_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_branch = LSTM(256)(inputs)\n",
    "    gru_branch = GRU(64)(inputs)\n",
    "    \n",
    "    def transformer_encoder(inputs, num_heads=4, key_dim=16, ff_dim=256, dropout=0.3):\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(\n",
    "            query=inputs, key=inputs, value=inputs\n",
    "        )\n",
    "        attn_output = Dropout(dropout)(attn_output)\n",
    "        attn_output = LayerNormalization(epsilon=1e-6)(attn_output + inputs)\n",
    "        \n",
    "        ffn_output = Dense(ff_dim, activation='relu')(attn_output)\n",
    "        ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "        ffn_output = Dropout(dropout)(ffn_output)\n",
    "        \n",
    "        return LayerNormalization(epsilon=1e-6)(ffn_output + attn_output)\n",
    "    \n",
    "    transformer_branch = transformer_encoder(inputs)\n",
    "    transformer_branch = GlobalAveragePooling1D()(transformer_branch)\n",
    "    concat = Concatenate()([lstm_branch, gru_branch, transformer_branch])\n",
    "    \n",
    "    dense = Dense(256, activation='relu')(concat)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    outputs = Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e19f9a",
   "metadata": {},
   "source": [
    "## **Step 5: Train the Model**  \n",
    "The model is compiled with the Adam optimizer and Mean Squared Error (MSE) loss function.  \n",
    "**Training Details:**  \n",
    "- **Batch Size:** 16  \n",
    "- **Epochs:** 20  \n",
    "- **Optimizer:** Adam (learning rate = 0.0001)  \n",
    "- **Validation Set:** 20% of data reserved for testing  \n",
    "During training, the model learns from historical data to minimize the forecasting error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 0.0061 - mae: 0.0516 - val_loss: 5.3307e-04 - val_mae: 0.0149\n",
      "Epoch 2/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - loss: 0.0010 - mae: 0.0211 - val_loss: 4.6257e-04 - val_mae: 0.0139\n",
      "Epoch 3/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - loss: 8.3251e-04 - mae: 0.0186 - val_loss: 3.9448e-04 - val_mae: 0.0115\n",
      "Epoch 4/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 7.4331e-04 - mae: 0.0175 - val_loss: 4.0186e-04 - val_mae: 0.0120\n",
      "Epoch 5/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 7.0191e-04 - mae: 0.0168 - val_loss: 4.8881e-04 - val_mae: 0.0144\n",
      "Epoch 6/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 6.8549e-04 - mae: 0.0163 - val_loss: 3.8687e-04 - val_mae: 0.0118\n",
      "Epoch 7/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 6.5878e-04 - mae: 0.0160 - val_loss: 4.1297e-04 - val_mae: 0.0122\n",
      "Epoch 8/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 5.9138e-04 - mae: 0.0156 - val_loss: 3.8213e-04 - val_mae: 0.0118\n",
      "Epoch 9/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 6.6904e-04 - mae: 0.0158 - val_loss: 3.7830e-04 - val_mae: 0.0114\n",
      "Epoch 10/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 6.2487e-04 - mae: 0.0154 - val_loss: 3.8315e-04 - val_mae: 0.0119\n",
      "Epoch 11/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 5.8633e-04 - mae: 0.0152 - val_loss: 3.9660e-04 - val_mae: 0.0123\n",
      "Epoch 12/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 5.7931e-04 - mae: 0.0152 - val_loss: 4.5414e-04 - val_mae: 0.0131\n",
      "Epoch 13/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 5.7912e-04 - mae: 0.0150 - val_loss: 4.3258e-04 - val_mae: 0.0123\n",
      "Epoch 14/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 5.5585e-04 - mae: 0.0150 - val_loss: 4.1537e-04 - val_mae: 0.0118\n",
      "Epoch 15/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 5.8326e-04 - mae: 0.0148 - val_loss: 3.6597e-04 - val_mae: 0.0108\n",
      "Epoch 16/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 5.7481e-04 - mae: 0.0148 - val_loss: 3.9403e-04 - val_mae: 0.0121\n",
      "Epoch 17/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 5.5266e-04 - mae: 0.0148 - val_loss: 3.8896e-04 - val_mae: 0.0113\n",
      "Epoch 18/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 5.0966e-04 - mae: 0.0145 - val_loss: 3.6459e-04 - val_mae: 0.0110\n",
      "Epoch 19/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 5.5082e-04 - mae: 0.0146 - val_loss: 3.6425e-04 - val_mae: 0.0107\n",
      "Epoch 20/20\n",
      "\u001b[1m4729/4729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 5.7881e-04 - mae: 0.0146 - val_loss: 3.7878e-04 - val_mae: 0.0112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x197963dfbd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model = define_model((X_train.shape[1], X_train.shape[2]))\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=16, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e15a4",
   "metadata": {},
   "source": [
    "## **Step 6: Save the Trained Model**  \n",
    "Once the model has been trained, it is saved for future predictions.  \n",
    "- The model is stored in `.keras` format under the `models` directory.  \n",
    "- This allows reloading the model without retraining from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models\\base_model.keras\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "model.save(MODEL_FILE)\n",
    "print(f\"Model saved to {MODEL_FILE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe45ef",
   "metadata": {},
   "source": [
    "## **Step 7: Evaluate Model Performance**  \n",
    "To assess model effectiveness, we compute key performance metrics:  \n",
    "- **R² Score (Coefficient of Determination):** Measures how well the model explains variance in PM2.5 levels.  \n",
    "- **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual values.  \n",
    "- **Root Mean Squared Error (RMSE):** Penalizes larger errors more than MAE.  \n",
    "**Results:**  \n",
    "- **Train R²:** 0.9423  \n",
    "- **Test R²:** 0.9538  \n",
    "- **Train MAE:** 0.0106, **Test MAE:** 0.0112  \n",
    "- **Train RMSE:** 0.0197, **Test RMSE:** 0.0195  \n",
    "These results indicate that the model has strong predictive accuracy on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2365/2365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m592/592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Train R²: 0.9423, MAE: 0.0106, RMSE: 0.0197\n",
      "Test R²: 0.9538, MAE: 0.0112, RMSE: 0.0195\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {r2_train:.4f}, MAE: {mae_train:.4f}, RMSE: {rmse_train:.4f}\")\n",
    "print(f\"Test R²: {r2_test:.4f}, MAE: {mae_test:.4f}, RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328da26e",
   "metadata": {},
   "source": [
    "## **Conclusion and Future Work**  \n",
    "**Key Takeaways:**  \n",
    "- The hybrid LSTM-GRU-Transformer model effectively captures air pollution trends.  \n",
    "- The high R² score suggests that the model generalizes well to new data.  \n",
    "- The combination of LSTM, GRU, and Transformer provides robust feature extraction.  \n",
    "\n",
    "**Future Enhancements:**  \n",
    "- Experiment with hyperparameter tuning to improve forecasting accuracy.  \n",
    "- Incorporate additional meteorological variables for better predictions.  \n",
    "- Apply advanced techniques such as attention mechanisms for improved performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
